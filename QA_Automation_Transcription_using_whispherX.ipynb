{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnthonyBadmus/cv/blob/main/QA_Automation_Transcription_using_whispherX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_speakers = 2 #@param {type:\"integer\"}\n",
        "\n",
        "language = 'any' #@param ['any', 'English']\n",
        "\n",
        "model_size = 'medium' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
        "\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'medium':\n",
        "  model_name += '.en'\n"
      ],
      "metadata": {
        "id": "buGt4moR5Mac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the PYTHONIOENCODING environment variable\n",
        "os.environ['PYTHONIOENCODING'] = 'UTF-8'"
      ],
      "metadata": {
        "id": "bxhjKwoh3NCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PYTHONUTF8=1"
      ],
      "metadata": {
        "id": "nVzMT1UJw7Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_dir = f'/content/drive/MyDrive/calls_updated/'\n",
        "input_dir = f'/content/drive/MyDrive/speech_to_text/growth_calls/'\n",
        "\n",
        "conv_dir = f'/content/drive/MyDrive/speech_to_text/convert/{datetime.date.today()}'\n",
        "\n",
        "transcribed_dir = f\"/content/drive/MyDrive/speech_to_text/transcribed/{datetime.date.today()}\"\n",
        "\n",
        "\n",
        "pickle_dir = f\"/content/drive/MyDrive/speech_to_text/pickle/{datetime.date.today()}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "POto_NHA356P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if directory exist, if not create it\n",
        "for path in [conv_dir,transcribed_dir,pickle_dir,pickle_dir2,transcribed_dir2]:\n",
        "  Path(path).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "B17ZOPkSIwxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert files to wav format\n",
        "import hashlib\n",
        "\n",
        "def file_hash(file_path) -> str:\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "\n",
        "def convert_to_wav(file_path):\n",
        "  file_name = file_path.stem\n",
        "  file_md5 = file_hash(file_path)\n",
        "  file_path = file_path.as_posix()\n",
        "  subprocess.call(['ffmpeg', '-i', file_path, f'{conv_dir}/{file_name}_{file_md5}.wav', '-y'])\n"
      ],
      "metadata": {
        "id": "pQbsmukPWrNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input\n",
        "from pathlib import Path\n",
        "# %cd drive/MyDrive/Calls\n",
        "# %ls\n",
        "\n",
        "\n",
        "files = Path(input_dir).iterdir()\n",
        "# next(files).as_posix()"
      ],
      "metadata": {
        "id": "yRUJX1XvnheC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %ls\n",
        "for x in files:\n",
        "  convert_to_wav(x)\n",
        "  # print(x)"
      ],
      "metadata": {
        "id": "xFI5FICcXGtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Object\n",
        "conv_files = Path(conv_dir).iterdir()"
      ],
      "metadata": {
        "id": "Ob0IYrMJqbXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload model for transcription\n",
        "model = whisper.load_model(model_size)"
      ],
      "metadata": {
        "id": "Vdbad9x5CHkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b3d7b2-72d1-4fd0-b035-1a64ca9d6c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:33<00:00, 45.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe(file_path):\n",
        "  file_path = file_path.as_posix()\n",
        "  result = model.transcribe(file_path)\n",
        "  segments = result[\"segments\"]\n",
        "  return segments"
      ],
      "metadata": {
        "id": "QHy8UZQzhtd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pickle(file_name):\n",
        "  with open(file_name, 'rb') as file:\n",
        "    return pickle.load(file)\n"
      ],
      "metadata": {
        "id": "qkMUxo4u-oX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write pickle files\n",
        "for call in conv_files:\n",
        "  segment = transcribe(call)\n",
        "  duration = timestamps(call)\n",
        "  write_pickle(Path(call).stem,(str(call), segment,duration))\n"
      ],
      "metadata": {
        "id": "C9m1Yn2cDShJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pkl_files = Path(pickle_dir).iterdir()\n",
        "parts = []\n",
        "for file in pkl_files:\n",
        "  if Path(file).suffix=='.pkl':\n",
        "     parts.append(read_pickle(file))\n"
      ],
      "metadata": {
        "id": "kvSWX-jJEjDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "\n",
        "for file in transcribed:\n",
        "  with open(file, 'r',encoding='utf-8') as f:\n",
        "    file_contents = f.read().strip()\n",
        "\n",
        "  g = {\n",
        "      'time':[i.strip().rsplit(maxsplit=1)[1] for i in file_contents.splitlines()[::2]],\n",
        "      'speaker':[i.strip().rsplit(maxsplit=1)[0] for i in file_contents.splitlines()[::2]],\n",
        "      'text':[i.strip() for i in file_contents.splitlines()[1::2]],\n",
        "       'call_id':Path(file).stem\n",
        "\n",
        "\n",
        "  }\n",
        "  df = pd.concat([df,pd.DataFrame(g)])"
      ],
      "metadata": {
        "id": "H-izATZ_ydTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path(path).stem"
      ],
      "metadata": {
        "id": "OUpwh1KnumlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using WhisperX"
      ],
      "metadata": {
        "id": "mTnoCUDKlDjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dependencies"
      ],
      "metadata": {
        "id": "He2jtvHwlVdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jca9oJUAlCy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056e36c2-1510-480d-ddc0-7f4f47620063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --q git+https://github.com/m-bain/whisperx.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DMTUmlhl92T",
        "outputId": "47ec3783-a056-4bcd-db36-e96c997941d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.1/760.1 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# import whisperx\n",
        "import gc\n",
        "import datetime\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "oyQAa_MClTv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get token"
      ],
      "metadata": {
        "id": "VsVP_SYZ9PdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/speech_to_text/hf_api_token.txt'\n",
        "\n",
        "def read_and_process_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "            print(content)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Token not found!\")"
      ],
      "metadata": {
        "id": "2v9DhI_mo79m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure resource"
      ],
      "metadata": {
        "id": "qa9XF_Pw9oy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# device = \"cuda\"\n",
        "device = \"cpu\"\n",
        "\n",
        "batch_size = 4 # reduce if low on GPU mem\n",
        "\n",
        "# compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "compute_type = \"int8\"\n"
      ],
      "metadata": {
        "id": "SIP8y2bJqW6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get audio"
      ],
      "metadata": {
        "id": "m1SKVUFE32dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get audio\n",
        "audio_file = \"/content/drive/My Drive/speech_to_text/convert/FM_5690811_8039654288.wav\"\n",
        "\n",
        "audio = whisperx.load_audio(audio_file)"
      ],
      "metadata": {
        "id": "9352w4FlrTSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load whisperX transcription model\n",
        "model = whisperx.load_model(\"large\", device, compute_type=compute_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg7j6NKJrdRt",
        "outputId": "05479b94-d65d-45dc-d9b4-fea2d20fbc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 16.9M/16.9M [00:01<00:00, 10.1MiB/s]\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu121. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcribe"
      ],
      "metadata": {
        "id": "Dcj3sjvaidde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(audio, batch_size=batch_size)\n",
        "# print(result[\"segments\"]) # before alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
        "\n",
        "\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCGO_RfGsId1",
        "outputId": "babb3780-1b12-49eb-a3a8-2b383cd92ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en (0.92) in first 30s of audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n",
            "100%|██████████| 360M/360M [00:05<00:00, 69.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segment audio (Speaker diarization)"
      ],
      "metadata": {
        "id": "vr5KO2j5itay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingFace_token = read_and_process_file(file_path)"
      ],
      "metadata": {
        "id": "3Eaw8F4NAMXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=huggingFace_token,\n",
        "                                             device=device)"
      ],
      "metadata": {
        "id": "sAZLaGzPsSab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarize_segments = diarize_model(audio, min_speakers=2, max_speakers=2)\n",
        "result = whisperx.assign_word_speakers(diarize_segments, result)\n"
      ],
      "metadata": {
        "id": "rxAbWijVsbLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_pickle(data, file_path):\n",
        "    with open(file_path, 'wb') as pickle_file:\n",
        "        pickle.dump(result, pickle_file)\n",
        "\n",
        "save_path = (f\"/content/drive/My Drive/speech_to_text/transcribed/conv.pkl_{datetime.date.today()}\")\n",
        "save_to_pickle(result, save_path)"
      ],
      "metadata": {
        "id": "9y2EVoplw9VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result df"
      ],
      "metadata": {
        "id": "IJ4sMDrDljIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_file_path = '/content/drive/My Drive/speech_to_text/transcribed/conv.pkl_2024-03-21'\n",
        "\n",
        "with open(pickle_file_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "\n",
        "conv = {}\n",
        "for v in data['segments']:\n",
        "    text = v['text'].strip()\n",
        "    spk = conv.get(v['speaker'])\n",
        "    if spk is None:\n",
        "        conv[v['speaker']]= [text]\n",
        "    else:\n",
        "\n",
        "        spk[0]+=(' '+text)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(conv)\n",
        "\n",
        "\n",
        "# df.to_csv(f\"/content/drive/My Drive/speech_to_text/transcribed/conv_df.csv_{datetime.date.today()}\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "lIyEjgrdpeZT",
        "outputId": "367a9505-c240-4b47-f3ff-37ce9f4dd5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          SPEAKER_00  \\\n",
              "0  Hello? Good afternoon. I'm fine, sir. My name ...   \n",
              "\n",
              "                                          SPEAKER_01  \n",
              "0  How are you? Yes. You know, I have a problem. ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6576798f-0186-412c-bee3-37673412b097\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SPEAKER_00</th>\n",
              "      <th>SPEAKER_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hello? Good afternoon. I'm fine, sir. My name ...</td>\n",
              "      <td>How are you? Yes. You know, I have a problem. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6576798f-0186-412c-bee3-37673412b097')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6576798f-0186-412c-bee3-37673412b097 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6576798f-0186-412c-bee3-37673412b097');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"SPEAKER_00\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Hello? Good afternoon. I'm fine, sir. My name is Ayomiko. My name is Ayomiko. I'm calling from B3 on behalf of Fair Money. Am I speaking with Mr. Sylvester Abbo? All right, sir. So I'm calling as regards your outstanding loan balance of $56,267, which has been due for 281 days. So we'd like to know why this payment has not been cleared. Okay, I understand you said that you really want to close this loan, but we have an offer for you, but it's just for a limited period of time. So we have a settlement offer of 45% of your original amount. So instead of paying that $56,237, you have to pay $25,313. So I really want you to get out of this offer, you understand? That's why I'm calling you right now. Instead of you paying that 56,267 Naira, you have to pay 25,330 Naira as of now. What happened to the remaining balance? This 25,330 Naira will clear off your loan. uh yes so it's an offer that uh that is available now at the moment and you're one of the lucky people do you understand that i'm benefiting out of this offer okay this of our last how many days sorry it could be like three days uh Okay, you can just try to give, you can give it a try. Maybe from now to Thursday, you can get the full payment of $25,330, okay? Okay. All right, sir. All right, sir. And also, did I say that your default details can be retrieved from credit bureau by fair money? So do you have the selling bank account details with you? All right, can you make sure you confirm the account name to be Fair Money and the first three letters of your name when you want to make this payment, okay? All right, so they said it's $25,330 on Thursday. Please walk towards it, give it a try, okay? All right, all right. And please do not pay cash to anyone who claims to be from Fair Money. Do not pay to anyone who claims to be from Fair Money. Also, don't quarrel and complain. Thank you so much for your time. Thank you.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SPEAKER_01\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How are you? Yes. You know, I have a problem. I spoke to your representative not quite long ago. Okay. I told him what happened. I made a payment of 2,000, I think about a week or so ago. But I told him that you were a size patient for me. But you know, we shall not be talking on the same thing. By January, I should be able to accept it. I don't understand what you are talking about. Oh, let me struggle now. If I'm able to see anything out there, if I'm not able, I'm prepared to sell through the entire something by January. I have, I have. I'm used to that one, no problem. Okay, okay, I'll try.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['SPEAKER_00'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "ZLBp5zSBIpmb",
        "outputId": "ade6df6a-8be7-4443-e4e5-c87c9496f324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello? Good afternoon. I'm fine, sir. My name is Ayomiko. My name is Ayomiko. I'm calling from B3 on behalf of Fair Money. Am I speaking with Mr. Sylvester Abbo? All right, sir. So I'm calling as regards your outstanding loan balance of $56,267, which has been due for 281 days. So we'd like to know why this payment has not been cleared. Okay, I understand you said that you really want to close this loan, but we have an offer for you, but it's just for a limited period of time. So we have a settlement offer of 45% of your original amount. So instead of paying that $56,237, you have to pay $25,313. So I really want you to get out of this offer, you understand? That's why I'm calling you right now. Instead of you paying that 56,267 Naira, you have to pay 25,330 Naira as of now. What happened to the remaining balance? This 25,330 Naira will clear off your loan. uh yes so it's an offer that uh that is available now at the moment and you're one of the lucky people do you understand that i'm benefiting out of this offer okay this of our last how many days sorry it could be like three days uh Okay, you can just try to give, you can give it a try. Maybe from now to Thursday, you can get the full payment of $25,330, okay? Okay. All right, sir. All right, sir. And also, did I say that your default details can be retrieved from credit bureau by fair money? So do you have the selling bank account details with you? All right, can you make sure you confirm the account name to be Fair Money and the first three letters of your name when you want to make this payment, okay? All right, so they said it's $25,330 on Thursday. Please walk towards it, give it a try, okay? All right, all right. And please do not pay cash to anyone who claims to be from Fair Money. Do not pay to anyone who claims to be from Fair Money. Also, don't quarrel and complain. Thank you so much for your time. Thank you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['SPEAKER_01'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "1HPNvsP4LiAR",
        "outputId": "f4caad85-ce8f-4380-b465-4b84b5788827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"How are you? Yes. You know, I have a problem. I spoke to your representative not quite long ago. Okay. I told him what happened. I made a payment of 2,000, I think about a week or so ago. But I told him that you were a size patient for me. But you know, we shall not be talking on the same thing. By January, I should be able to accept it. I don't understand what you are talking about. Oh, let me struggle now. If I'm able to see anything out there, if I'm not able, I'm prepared to sell through the entire something by January. I have, I have. I'm used to that one, no problem. Okay, okay, I'll try.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LQGWOP6IzEpn"
      }
    }
  ]
}